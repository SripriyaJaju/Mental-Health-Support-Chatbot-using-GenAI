{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n# Install dependencies\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:16:28.198662Z","iopub.execute_input":"2025-04-13T05:16:28.199038Z","iopub.status.idle":"2025-04-13T05:20:49.129025Z","shell.execute_reply.started":"2025-04-13T05:16:28.199006Z","shell.execute_reply":"2025-04-13T05:20:49.127950Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Core libraries\nimport math\nimport torch\nimport wandb\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import login\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom trl import SFTTrainer, SFTConfig\n# Unsloth utilities\nfrom unsloth import FastModel, FastLanguageModel, is_bfloat16_supported\nfrom unsloth.chat_templates import (\n    get_chat_template,\n    standardize_data_formats,\n    train_on_responses_only\n)\n\n# Kaggle secrets (if running on Kaggle)\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:20:57.187243Z","iopub.execute_input":"2025-04-13T05:20:57.187567Z","iopub.status.idle":"2025-04-13T05:21:27.367527Z","shell.execute_reply.started":"2025-04-13T05:20:57.187541Z","shell.execute_reply":"2025-04-13T05:21:27.366583Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-36183deb6ac9>:10: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n\nPlease restructure your imports with 'import unsloth' at the top of your file.\n  from unsloth import FastModel, FastLanguageModel, is_bfloat16_supported\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 0. Authenticate to Hugging Face & W&B\n# ---------------------------------------------------------------------------- #\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_Token\")\nwandb_token = user_secrets.get_secret(\"wnb\")\n\nlogin(hf_token)\nwandb.login(key=wandb_token)\nrun = wandb.init(\n    project='Mental-Health-Support-Using-CBT-Fine-tune',\n    job_type=\"training\",\n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:21:32.214236Z","iopub.execute_input":"2025-04-13T05:21:32.214543Z","iopub.status.idle":"2025-04-13T05:21:44.764650Z","shell.execute_reply.started":"2025-04-13T05:21:32.214521Z","shell.execute_reply":"2025-04-13T05:21:44.763889Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msripriyajaju\u001b[0m (\u001b[33msripriyajaju-methodist-college-of-engineering-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250413_052138-qw9843rq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune/runs/qw9843rq' target=\"_blank\">glad-wood-11</a></strong> to <a href='https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune' target=\"_blank\">https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune/runs/qw9843rq' target=\"_blank\">https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune/runs/qw9843rq</a>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# 1. Load + Subsample + Split Dataset\n# ---------------------------------------------------------------------------- #\nraw_full = load_dataset(\n    \"saarib2405/Cactus-Mental-Health-dataset\",\n    split=\"train\",\n    trust_remote_code=True\n)\n\n# Shuffle and select 1,100 examples\nraw_small = raw_full.shuffle(seed=3407).select(range(1100))\n\n# 90% train (1,000) / 10% eval (100)\nsplit = raw_small.train_test_split(test_size=0.1, seed=3407)\ntrain_raw, eval_raw = split[\"train\"], split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:21:48.817567Z","iopub.execute_input":"2025-04-13T05:21:48.818062Z","iopub.status.idle":"2025-04-13T05:21:57.019030Z","shell.execute_reply.started":"2025-04-13T05:21:48.818023Z","shell.execute_reply":"2025-04-13T05:21:57.018403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"cactus.json:   0%|          | 0.00/270M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2f59d173764062bf8162bbf5dc25ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/31577 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ee3c7bc0af49978fe94554189d17a1"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 2. Convert dialogues to input-output pairs\n# ---------------------------------------------------------------------------- #\ndef split_dialogue_into_pairs(example):\n    lines = example[\"dialogue\"].split('\\n')\n    pairs, user_msg = [], None\n    for line in lines:\n        if line.startswith(\"Client:\"):\n            user_msg = line[len(\"Client: \"):].strip()\n        elif line.startswith(\"Counselor:\") and user_msg:\n            assistant_msg = line[len(\"Counselor: \"):].strip()\n            pairs.append({'input': user_msg, 'output': assistant_msg})\n            user_msg = None\n    return pairs\n\ndef generate_pairs_dataset(ds):\n    all_pairs = []\n    for ex in ds:\n        all_pairs.extend(split_dialogue_into_pairs(ex))\n    return Dataset.from_list(all_pairs)\n\ndef convert_dataset(pairs_ds):\n    converted = []\n    for rec in pairs_ds:\n        user_content = rec['input']\n        assistant_content = rec['output']\n        # Append CBT metadata if present\n        for key in ('cbt_technique','cbt_plan'):\n            val = rec.get(key, '')\n            if val:\n                user_content += f\"\\n\\nAdditional CBT Context:\\n{key.replace('_',' ').title()}: {val}\"\n        conv = [\n            {'role':'user','content':user_content},\n            {'role':'assistant','content':assistant_content}\n        ]\n        converted.append({'conversations': conv})\n    return Dataset.from_list(converted)\n\n# Prepare train & eval pairs\ntrain_pairs = generate_pairs_dataset(train_raw)\neval_pairs  = generate_pairs_dataset(eval_raw)\n\ntrain_conv = convert_dataset(train_pairs)\neval_conv  = convert_dataset(eval_pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:22:01.719183Z","iopub.execute_input":"2025-04-13T05:22:01.719479Z","iopub.status.idle":"2025-04-13T05:22:02.869542Z","shell.execute_reply.started":"2025-04-13T05:22:01.719455Z","shell.execute_reply":"2025-04-13T05:22:02.868632Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 3. Tokenizer & Chat Template\n# ---------------------------------------------------------------------------- #\n# Load base Gemma-3 tokenizer & model in 4-bit\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name=\"unsloth/gemma-3-4b-it\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n    full_finetuning=False,\n)\n\n# Apply the chat template for Gemma-3\ntokenizer = get_chat_template(tokenizer, chat_template=\"gemma-3\")\n\n# Standardize and apply template\ntrain_ds = standardize_data_formats(train_conv).map(\n    lambda ex: {\"text\": tokenizer.apply_chat_template(ex[\"conversations\"])},\n    batched=True\n)\neval_ds = standardize_data_formats(eval_conv).map(\n    lambda ex: {\"text\": tokenizer.apply_chat_template(ex[\"conversations\"])},\n    batched=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:22:08.377621Z","iopub.execute_input":"2025-04-13T05:22:08.377965Z","iopub.status.idle":"2025-04-13T05:22:53.536872Z","shell.execute_reply.started":"2025-04-13T05:22:08.377936Z","shell.execute_reply":"2025-04-13T05:22:53.536086Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9959fea482d4f57ab5d760b46593393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4febec8cf70d4e9ebab2108fd722f5e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b49cbf5e7f549859bcc5641b2c9bd90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c05e50a54f60461faef70c23a47810a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e43c38e6bb242e08c9aa905465b4d54"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88912e7a5de4e249e81316353161596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f3222052b54bfba646a39ab7ae2e3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e637504307504469be4cd4fcc0b731e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b010a1459914e5aa62dd3e187029549"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c07f0bfb9389498d85a4af6acc6bc78d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Standardizing formats (num_proc=4):   0%|          | 0/14431 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c87d69b5ec04bd7a007147b6833a61c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14431 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b56cd34fd1644e9b80635c9ac796a20b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Standardizing formats (num_proc=4):   0%|          | 0/1590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd91cb0c8d84018962907c91b3416eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3baec3d4fd143d58a35c39586b03183"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ---------------------------------------------------------------------------- #\n# 4. PEFT (LoRA) Setup\n# ---------------------------------------------------------------------------- #\nmodel = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers=False,\n    finetune_language_layers=True,\n    finetune_attention_modules=True,\n    finetune_mlp_modules=True,\n    r=8,\n    lora_alpha=8,\n    lora_dropout=0.0,\n    bias=\"none\",\n    random_state=3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:22:57.750073Z","iopub.execute_input":"2025-04-13T05:22:57.750390Z","iopub.status.idle":"2025-04-13T05:23:02.635599Z","shell.execute_reply.started":"2025-04-13T05:22:57.750365Z","shell.execute_reply":"2025-04-13T05:23:02.634899Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.language_model.model` require gradients\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ---------------------------------------------------------------------------- #\n# 5. Trainer Configuration\n# ---------------------------------------------------------------------------- #\nfrom trl import SFTTrainer\nfrom transformers import DataCollatorForLanguageModeling\nfrom unsloth import FastLanguageModel\n\n# Make sure you have a proper tokenizer with padding capability\n# If using Gemma, it should be:\n# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n\n# Create data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # For causal LM\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    data_collator=data_collator,  # Add this line\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=8,\n        #gradient_accumulation_steps=2,\n        warmup_steps=5,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=1,\n        evaluation_strategy=\"steps\",\n        eval_steps=10,\n        save_strategy=\"no\",\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        report_to=\"wandb\",\n    ),\n)\n\nfrom unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [{\n        \"type\" : \"text\",\n        \"text\" : \"I feel like I am a very bad person \",\n    }]\n}]\ntext = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n)\noutputs = model.generate(\n    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 256, # Increase for longer outputs!\n    # Recommended Gemma-3 settings!\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:23:23.112258Z","iopub.execute_input":"2025-04-13T05:23:23.112607Z","iopub.status.idle":"2025-04-13T05:24:48.584413Z","shell.execute_reply.started":"2025-04-13T05:23:23.112583Z","shell.execute_reply":"2025-04-13T05:24:48.583596Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/14431 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfd296ec2c24a53954aac4f6d848035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/1590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c53ae174ab74d158d244418d47b6603"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['<bos><start_of_turn>user\\nI feel like I am a very bad person<end_of_turn>\\n<start_of_turn>model\\nOkay, that\\'s a really important and difficult thing to be feeling. It takes a lot of courage to admit that, and I want to acknowledge that you’re here talking about it – that’s a significant first step. It\\'s completely understandable to feel like you’re a “bad person” when you’re struggling with feelings of guilt, shame, or regret. Let’s unpack this a bit. \\n\\nFirst, let’s just be clear: **You are not alone.** Many people experience periods where they question their worth and feel like they’ve done something fundamentally wrong.  It doesn’t automatically make you a “bad person.” It’s often a sign that you\\'re grappling with difficult emotions and recognizing that you’ve acted in a way that’s hurtful, or that doesn\\'t align with your values.\\n\\nHere\\'s a breakdown of things to consider and what we can do:\\n\\n**1. Let\\'s Explore What\\'s Driving This Feeling:**\\n\\n* **What specifically are you feeling bad about?** Can you be more specific?  It\\'s important to pinpoint exactly *what* actions or thoughts are leading you to feel this way.  Instead of saying \"I\\'m a bad']"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 6. Train & Evaluate\n# ---------------------------------------------------------------------------- #\ntry:\n    trainer_stats = trainer.train()\n    eval_results = trainer.evaluate()\n    eval_loss = eval_results[\"eval_loss\"]\n    perplexity = math.exp(eval_loss)\n\n    print(f\"Perplexity on held-out data: {perplexity:.2f}\")\n    wandb.log({\"perplexity\": perplexity})\nexcept Exception as e:\n    print(f\"Error during training: {str(e)}\")\n    # Additional debug information\n    print(f\"Tokenizer type: {type(tokenizer)}\")\n    print(f\"Tokenizer has pad method: {hasattr(tokenizer, 'pad')}\")\n    print(f\"Processor type: {type(processor) if 'processor' in locals() else 'No processor defined'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:25:08.412142Z","iopub.execute_input":"2025-04-13T05:25:08.412512Z","iopub.status.idle":"2025-04-13T05:52:11.944173Z","shell.execute_reply.started":"2025-04-13T05:25:08.412476Z","shell.execute_reply":"2025-04-13T05:52:11.943222Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 14,431 | Num Epochs = 1 | Total steps = 100\nO^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 24:31, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.172900</td>\n      <td>2.949324</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.931700</td>\n      <td>2.015918</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.826600</td>\n      <td>1.760409</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.647900</td>\n      <td>1.586659</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.561200</td>\n      <td>1.540449</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.496900</td>\n      <td>1.517000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.463200</td>\n      <td>1.505512</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.391700</td>\n      <td>1.492764</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.451500</td>\n      <td>1.486986</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.521600</td>\n      <td>1.483139</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:51]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity on held-out data: 4.41\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:54:10.253275Z","iopub.execute_input":"2025-04-13T05:54:10.254362Z","iopub.status.idle":"2025-04-13T05:54:11.639993Z","shell.execute_reply.started":"2025-04-13T05:54:10.254330Z","shell.execute_reply":"2025-04-13T05:54:11.639317Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▂▂▂▁▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁████▇▇▇█▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁████▇▇▇█▇▇</td></tr><tr><td>perplexity</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>██▆▃▃▄▃▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▄▅▇██▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.48314</td></tr><tr><td>eval/runtime</td><td>112.1722</td></tr><tr><td>eval/samples_per_second</td><td>14.175</td></tr><tr><td>eval/steps_per_second</td><td>0.891</td></tr><tr><td>perplexity</td><td>4.40676</td></tr><tr><td>total_flos</td><td>3563667592247808.0</td></tr><tr><td>train/epoch</td><td>0.11086</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>0.86125</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.5216</td></tr><tr><td>train_loss</td><td>2.06401</td></tr><tr><td>train_runtime</td><td>1508.777</td></tr><tr><td>train_samples_per_second</td><td>1.06</td></tr><tr><td>train_steps_per_second</td><td>0.066</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">glad-wood-11</strong> at: <a href='https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune/runs/qw9843rq' target=\"_blank\">https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune/runs/qw9843rq</a><br> View project at: <a href='https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune' target=\"_blank\">https://wandb.ai/sripriyajaju-methodist-college-of-engineering-and-technology/Mental-Health-Support-Using-CBT-Fine-tune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250413_052138-qw9843rq/logs</code>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# 7. Quick Generation Demo (optional)\n# ---------------------------------------------------------------------------- #\nmessages = [\n    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\n     \"Respond as a compassionate CBT expert, offering concise, empathetic guidance with a practical CBT exercise (max 100 words, one sentence) for mental well-being, acknowledging emotions without medical advice, and conclude with an encouraging sentence.\"}]},\n    {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\n     \"I'm having trouble communicating with my partner. We keep misinterpreting each other.\"}]}\n]\ntext = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\noutputs = model.generate(\n    **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n    max_new_tokens=256,\n    temperature=1.0, top_p=0.95, top_k=64\n)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:59:22.978588Z","iopub.execute_input":"2025-04-13T05:59:22.978977Z","iopub.status.idle":"2025-04-13T05:59:39.972751Z","shell.execute_reply.started":"2025-04-13T05:59:22.978949Z","shell.execute_reply":"2025-04-13T05:59:39.971869Z"}},"outputs":[{"name":"stdout","text":"[\"user\\nRespond as a compassionate CBT expert, offering concise, empathetic guidance with a practical CBT exercise (max 100 words, one sentence) for mental well-being, acknowledging emotions without medical advice, and conclude with an encouraging sentence.\\n\\nI'm having trouble communicating with my partner. We keep misinterpreting each other.\\nmodel\\nIt’s tough to have those feelings when important relationships are strained. It’s good that you’re willing to share this with me. How about practicing a simple technique to reduce feelings of guilt and focus on positive communication with your partner? It’s okay to be a little anxious, but these efforts can help you find a new path forward. It’s your own path, and you deserve it.\"]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"messages = [{\n    \"role\": \"user\",\n    \"content\": [{\"type\" : \"text\", \"text\" : \"I lost my job\",}]\n}]\ntext = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n)\n\nfrom transformers import TextStreamer\n_ = model.generate(\n    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 258, # Increase for longer outputs!\n    # Recommended Gemma-3 settings!\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n    streamer = TextStreamer(tokenizer, skip_prompt = True),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:59:39.974085Z","iopub.execute_input":"2025-04-13T05:59:39.974349Z","iopub.status.idle":"2025-04-13T05:59:42.427934Z","shell.execute_reply.started":"2025-04-13T05:59:39.974328Z","shell.execute_reply":"2025-04-13T05:59:42.427180Z"}},"outputs":[{"name":"stdout","text":"That sounds tough. How long ago did this happen?<end_of_turn>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:00:47.101064Z","iopub.execute_input":"2025-04-13T06:00:47.101412Z","iopub.status.idle":"2025-04-13T06:00:51.824956Z","shell.execute_reply.started":"2025-04-13T06:00:47.101387Z","shell.execute_reply":"2025-04-13T06:00:51.823864Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:00:54.119722Z","iopub.execute_input":"2025-04-13T06:00:54.120121Z","iopub.status.idle":"2025-04-13T06:01:00.477814Z","shell.execute_reply.started":"2025-04-13T06:00:54.120091Z","shell.execute_reply":"2025-04-13T06:01:00.476838Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=732d1358e7e0556d9421e33963fa50b32151f945884c65edb87ad52c76dc589e\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\nimport torch\n\n# Load metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\n# Use a small eval sample (increase range for more thorough eval)\nval_data = eval_conv.select(range(100))\n\npredictions = []\nreferences = []\n\nfor example in tqdm(val_data):\n    prompt = tokenizer.apply_chat_template(example[\"conversations\"], add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=100,\n            temperature=1.0,\n            top_p=0.95,\n            top_k=64,\n        )\n\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    # Ground truth assistant response\n    reference = example[\"conversations\"][-1][\"content\"]\n\n    predictions.append(generated_text)\n    references.append(reference)\n\n# Compute BLEU (no need to split)\nbleu_result = bleu_metric.compute(\n    predictions=predictions,\n    references=[[ref] for ref in references],  # wrap each reference in a list\n)\n\n# Compute ROUGE (same format, no changes)\nrouge_result = rouge_metric.compute(\n    predictions=predictions,\n    references=references\n)\n\nprint(\"BLEU Score:\", bleu_result[\"bleu\"])\nprint(\"ROUGE-L Score:\", rouge_result[\"rougeL\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:01:03.805139Z","iopub.execute_input":"2025-04-13T06:01:03.805479Z","iopub.status.idle":"2025-04-13T06:12:06.043365Z","shell.execute_reply.started":"2025-04-13T06:01:03.805454Z","shell.execute_reply":"2025-04-13T06:12:06.042442Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3973cadb03ff4b08b689b0021e181793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255581b0f3db424d806193d067f786ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"517194ee71394afc9dc86e20fa610f4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1351e89d6644479a815534fef2323229"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 100/100 [10:59<00:00,  6.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"BLEU Score: 0.3373920234722069\nROUGE-L Score: 0.5132544771268881\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}